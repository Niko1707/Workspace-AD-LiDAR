{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'path_list_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 292\u001b[0m\n\u001b[1;32m    285\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args()\n\u001b[1;32m    287\u001b[0m logging\u001b[39m.\u001b[39mbasicConfig(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%(asctime)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(levelname)-8s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(message)s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m    288\u001b[0m                     level\u001b[39m=\u001b[39mlogging\u001b[39m.\u001b[39mDEBUG,\n\u001b[1;32m    289\u001b[0m                     datefmt\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    291\u001b[0m train(args\u001b[39m.\u001b[39mdataset_folder,\n\u001b[0;32m--> 292\u001b[0m       args\u001b[39m.\u001b[39;49mpath_list_files,\n\u001b[1;32m    293\u001b[0m       args\u001b[39m.\u001b[39moutput_folder,\n\u001b[1;32m    294\u001b[0m       args\u001b[39m.\u001b[39mnumber_of_points,\n\u001b[1;32m    295\u001b[0m       args\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m    296\u001b[0m       args\u001b[39m.\u001b[39mepochs,\n\u001b[1;32m    297\u001b[0m       args\u001b[39m.\u001b[39mlearning_rate,\n\u001b[1;32m    298\u001b[0m       args\u001b[39m.\u001b[39mweighing_method,\n\u001b[1;32m    299\u001b[0m       args\u001b[39m.\u001b[39mbeta,\n\u001b[1;32m    300\u001b[0m       args\u001b[39m.\u001b[39mnumber_of_workers,\n\u001b[1;32m    301\u001b[0m       args\u001b[39m.\u001b[39mmodel_checkpoint,\n\u001b[1;32m    302\u001b[0m       args\u001b[39m.\u001b[39mc_sample)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'path_list_files'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from progressbar import progressbar\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from B0_Dataset.dataset import SemanticKittiDataset\n",
    "from D0_Modeling.model import SegmentationPointNet\n",
    "import logging\n",
    "import datetime\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import warnings\n",
    "from utils import *\n",
    "import glob\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    logging.info(f\"cuda available\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    logging.info(f\"cuda not available\")\n",
    "    device = 'cpu'\n",
    "\n",
    "\n",
    "def train(\n",
    "        dataset_folder,\n",
    "        path_list_files,\n",
    "        output_folder,\n",
    "        n_points,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        learning_rate,\n",
    "        weighing_method,\n",
    "        beta,\n",
    "        number_of_workers,\n",
    "        model_checkpoint,\n",
    "        c_sample=False):\n",
    "    start_time = time.time()\n",
    "    logging.info(f\"Weighing method: {weighing_method}\")\n",
    "    logging.info(f\"Constrained sampling: {c_sample}\")\n",
    "\n",
    "    # Tensorboard location and plot names\n",
    "    now = datetime.datetime.now()\n",
    "    location = 'pointNet/runs/tower_detec/' + str(n_points) + 'p/'\n",
    "\n",
    "    # Datasets train / val / test\n",
    "    with open(os.path.join(path_list_files, 'train_seg_files.txt'), 'r') as f:\n",
    "        train_files = f.read().splitlines()\n",
    "    with open(os.path.join(path_list_files, 'val_seg_files.txt'), 'r') as f:\n",
    "        val_files = f.read().splitlines()\n",
    "\n",
    "    writer_train = SummaryWriter(location + now.strftime(\"%m-%d-%H:%M\") + 'seg_train')\n",
    "    writer_val = SummaryWriter(location + now.strftime(\"%m-%d-%H:%M\") + 'seg_val')\n",
    "    logging.info(f\"Tensorboard runs: {writer_train.get_logdir()}\")\n",
    "\n",
    "    # Initialize datasets\n",
    "    train_dataset = SemanticKittiDataset(dataset_folder=dataset_folder,\n",
    "                                 task='segmentation', number_of_points=n_points,\n",
    "                                 files=train_files,\n",
    "                                 fixed_num_points=True,\n",
    "                                 c_sample=c_sample)\n",
    "    val_dataset = SemanticKittiDataset(dataset_folder=dataset_folder,\n",
    "                               task='segmentation', number_of_points=n_points,\n",
    "                               files=val_files,\n",
    "                               fixed_num_points=True,\n",
    "                               c_sample=c_sample)\n",
    "\n",
    "    logging.info(f'Towers PC in train: {train_dataset.len_towers}')\n",
    "    logging.info(f'Landscape PC in train: {train_dataset.len_landscape}')\n",
    "    logging.info(\n",
    "        f'Proportion towers/landscape: {round((train_dataset.len_towers / (train_dataset.len_towers + train_dataset.len_landscape)) * 100, 3)}%')\n",
    "    logging.info(f'Towers PC in val: {val_dataset.len_towers}')\n",
    "    logging.info(f'Landscape PC in val: {val_dataset.len_landscape}')\n",
    "    logging.info(\n",
    "        f'Proportion towers/landscape: {round((val_dataset.len_towers / (val_dataset.len_towers + val_dataset.len_landscape)) * 100, 3)}%')\n",
    "    logging.info(f'Samples for training: {len(train_dataset)}')\n",
    "    logging.info(f'Samples for validation: {len(val_dataset)}')\n",
    "    logging.info(f'Task: {train_dataset.task}')\n",
    "\n",
    "    # Dataloaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=number_of_workers,\n",
    "                                                   drop_last=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True,\n",
    "                                                 num_workers=number_of_workers,\n",
    "                                                 drop_last=True)\n",
    "\n",
    "    model = SegmentationPointNet(num_classes=train_dataset.NUM_SEGMENTATION_CLASSES,\n",
    "                                       point_dimension=train_dataset.POINT_DIMENSION)\n",
    "    model.to(device)\n",
    "\n",
    "    # print model and parameters\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    # print(table)\n",
    "    logging.info(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if model_checkpoint:\n",
    "        print('Loading checkpoint')\n",
    "        checkpoint = torch.load(model_checkpoint)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    for epoch in progressbar(range(epochs), redirect_stdout=True):\n",
    "        epoch_train_loss = []\n",
    "        epoch_train_acc = []\n",
    "        all_weights = []\n",
    "        epoch_val_loss = []\n",
    "        epoch_val_acc = []\n",
    "        epoch_train_acc_w = []\n",
    "        epoch_val_acc_w = []\n",
    "        detected_positive = []\n",
    "        detected_negative = []\n",
    "        targets_pos = []\n",
    "        targets_neg = []\n",
    "\n",
    "        if epochs_since_improvement == 10:\n",
    "            adjust_learning_rate(optimizer, 0.5)\n",
    "        elif epoch == 10:\n",
    "            adjust_learning_rate(optimizer, 0.5)\n",
    "\n",
    "        # --------------------------------------------- train loop ---------------------------------------------\n",
    "        for data in train_dataloader:\n",
    "            points, targets, filenames = data  # [7557, batch, dims], [4]\n",
    "\n",
    "            points = points.view(batch_size, n_points, -1).to(device)  # [batch, n_samples, dims]\n",
    "            targets = targets.view(batch_size, -1).to(device)  # [batch, n_samples]\n",
    "\n",
    "            # Pytorch accumulates gradients. We need to clear them out before each instance\n",
    "            optimizer.zero_grad()\n",
    "            model = model.train()\n",
    "            preds, feature_transform = model(points)\n",
    "\n",
    "            preds = preds.view(-1, train_dataset.NUM_SEGMENTATION_CLASSES)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            # get weights for imbalanced loss\n",
    "            points_tower = (np.array(targets.cpu()) == np.ones(len(targets))).sum()\n",
    "            points_landscape = (np.array(targets.cpu()) == np.zeros(len(targets))).sum()\n",
    "\n",
    "            if not points_tower:\n",
    "                points_tower = 100\n",
    "                points_landscape = 4000\n",
    "\n",
    "            c_weights = get_weights4class(weighing_method,\n",
    "                                          n_classes=2,\n",
    "                                          samples_per_cls=[points_landscape, points_tower],\n",
    "                                          beta=beta).to(device)\n",
    "            sample_weights = get_weights4sample(c_weights.cpu(), labels=targets).numpy()\n",
    "\n",
    "            identity = torch.eye(feature_transform.shape[-1]).to(device)\n",
    "\n",
    "            regularization_loss = torch.norm(identity - torch.bmm(feature_transform, feature_transform.transpose(2, 1)))\n",
    "            loss = F.nll_loss(preds, targets, weight=c_weights) + 0.001 * regularization_loss\n",
    "            epoch_train_loss.append(loss.cpu().item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = preds.data.max(1)[1]\n",
    "            corrects = preds.eq(targets.data).cpu().sum()\n",
    "            accuracy = corrects.item() / float(targets.shape[0])\n",
    "            accuracy_w = balanced_accuracy_score(targets.cpu(), preds.cpu(), sample_weight=sample_weights)\n",
    "            epoch_train_acc_w.append(accuracy_w)\n",
    "            epoch_train_acc.append(accuracy)\n",
    "            all_weights.append(c_weights[1].cpu())\n",
    "\n",
    "        # --------------------------------------------- val loop ---------------------------------------------\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                points, targets, filenames = data  # [7557, 4, 12]\n",
    "\n",
    "                points = points.view(batch_size, n_points, -1).to(device)  # [batch, n_samples, dims]\n",
    "                targets = targets.view(batch_size, -1).to(device)  # [batch, n_samples]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                model = model.eval()\n",
    "                preds, feature_transform = model(points)\n",
    "\n",
    "                preds = preds.view(-1, train_dataset.NUM_SEGMENTATION_CLASSES)\n",
    "                targets = targets.view(-1)\n",
    "\n",
    "                # get weights for imbalanced loss\n",
    "                points_tower = (np.array(targets.cpu()) == np.ones(len(targets))).sum()\n",
    "                points_landscape = (np.array(targets.cpu()) == np.zeros(len(targets))).sum()\n",
    "\n",
    "                # weights\n",
    "                c_weights = get_weights4class(weighing_method,\n",
    "                                              n_classes=2,\n",
    "                                              samples_per_cls=[points_landscape, points_tower],\n",
    "                                              beta=beta).to(device)\n",
    "                sample_weights = get_weights4sample(c_weights.cpu(), labels=targets).numpy()\n",
    "\n",
    "                loss = F.nll_loss(preds, targets, weight=c_weights)\n",
    "                epoch_val_loss.append(loss.cpu().item())\n",
    "                preds = preds.data.max(1)[1]\n",
    "                corrects = preds.eq(targets.data).cpu().sum()\n",
    "\n",
    "                # sum of targets in batch\n",
    "                targets_pos.append((np.array(targets.cpu()) == np.ones(len(targets))).sum())\n",
    "                targets_neg.append((np.array(targets.cpu()) == np.zeros(len(targets))).sum())\n",
    "                detected_positive.append(\n",
    "                    (np.array(preds.cpu()) == np.ones(len(preds))).sum())  # bool with positions of 1s\n",
    "                detected_negative.append(\n",
    "                    (np.array(preds.cpu()) == np.zeros(len(preds))).sum())  # bool with positions of 0s\n",
    "\n",
    "                accuracy = corrects.item() / float(targets.shape[0])\n",
    "                accuracy_w = balanced_accuracy_score(targets.cpu(), preds.cpu(), sample_weight=sample_weights)\n",
    "                epoch_val_acc_w.append(accuracy_w)\n",
    "                epoch_val_acc.append(accuracy)\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        # Tensorboard\n",
    "        writer_train.add_scalar('loss', np.mean(epoch_train_loss), epoch)\n",
    "        writer_val.add_scalar('loss', np.mean(epoch_val_loss), epoch)\n",
    "        writer_train.add_scalar('mean_detected_positive', np.mean(targets_pos), epoch)\n",
    "        writer_val.add_scalar('mean_detected_positive', np.mean(detected_positive), epoch)\n",
    "        writer_train.add_scalar('mean_detected_negative', np.mean(targets_neg), epoch)\n",
    "        writer_val.add_scalar('mean_detected_negative', np.mean(detected_negative), epoch)\n",
    "        writer_train.add_scalar('accuracy_weighted', np.mean(epoch_train_acc_w), epoch)\n",
    "        writer_val.add_scalar('accuracy_weighted', np.mean(epoch_val_acc_w), epoch)\n",
    "        writer_train.add_scalar('accuracy', np.mean(epoch_train_acc), epoch)\n",
    "        writer_val.add_scalar('accuracy', np.mean(epoch_val_acc), epoch)\n",
    "        writer_val.add_scalar('epochs_since_improvement', epochs_since_improvement, epoch)\n",
    "        writer_val.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        writer_val.add_scalar('c_weights', np.mean(all_weights), epoch)\n",
    "\n",
    "        writer_train.flush()\n",
    "        writer_val.flush()\n",
    "\n",
    "        if np.mean(epoch_val_loss) < best_vloss:\n",
    "            # Save checkpoint\n",
    "            name = now.strftime(\"%m-%d-%H:%M\") + '_seg'\n",
    "            save_checkpoint(name, epoch, epochs_since_improvement, model, optimizer, accuracy, batch_size,\n",
    "                            learning_rate, n_points, weighing_method)\n",
    "            epochs_since_improvement = 0\n",
    "            best_vloss = np.mean(epoch_val_loss)\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "    # if output_folder:\n",
    "    #     if not os.path.isdir(output_folder):\n",
    "    #         os.mkdir(output_folder)\n",
    "    # plot_losses(train_loss, test_loss, save_to_file=os.path.join(output_folder, 'loss_plot.png'))\n",
    "    # plot_accuracies(train_acc, test_acc, save_to_file=os.path.join(output_folder, 'accuracy_plot.png'))\n",
    "    print(\"--- TOTAL TIME: %s min ---\" % (round((time.time() - start_time) / 60, 3)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset_folder', type=str, default='/Users/nikolai/Downloads/dataset/sequences/', help='dataset folder')\n",
    "    parser.add_argument('--path_list_files', type=str, default='/Users/nikolai/Downloads/UPC/VSC/Project/listfiles/', help='list files folder')\n",
    "    parser.add_argument('--output_folder', type=str, default='/Users/nikolai/Downloads/UPC/VSC/Project/output/', help='output folder')\n",
    "    parser.add_argument('--number_of_points', type=int, default=4000, help='number of points per cloud')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=50, help='number of epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='learning rate')\n",
    "    parser.add_argument('--weighing_method', type=str, default='EFS', help='sample weighing method: ISNS or INS or EFS')\n",
    "    parser.add_argument('--beta', type=float, default=0.999, help='model checkpoint path')\n",
    "    parser.add_argument('--number_of_workers', type=int, default=4, help='number of workers for the dataloader')\n",
    "    parser.add_argument('--model_checkpoint', type=str, default='', help='model checkpoint path')\n",
    "    parser.add_argument('--c_sample', type=bool, default=False, help='use constrained sampling')\n",
    "    parser.add_argument('-f', '--file', required=False) \n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "                        level=logging.DEBUG,\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    train(args.dataset_folder,\n",
    "          args.path_list_files,\n",
    "          args.output_folder,\n",
    "          args.number_of_points,\n",
    "          args.batch_size,\n",
    "          args.epochs,\n",
    "          args.learning_rate,\n",
    "          args.weighing_method,\n",
    "          args.beta,\n",
    "          args.number_of_workers,\n",
    "          args.model_checkpoint,\n",
    "          args.c_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69bb5bf7338e4323a6009980fdae7212bdac3646f7ada4395191108fa1909851"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
