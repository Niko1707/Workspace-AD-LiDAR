{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer for revised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opt():\n",
    "    hparamDatasetPath = \"/Users/nikolai/Downloads/UPC/VSC/Project/dataset/sequences\"\n",
    "    hparamYamlConfigPath = \"/Users/nikolai/Downloads/UPC/VSC/Project/Workspace-AD-LiDAR/F0_Visualization/semantic-kitti-api/config/semantic-kitti.yaml\"\n",
    "    hparamNumPoints = 4000\n",
    "    hparamNumberOfClasses = 20\n",
    "    #hparamClassChoice = 'bus'\n",
    "    hparamDatasetSequence = '04'\n",
    "    hparamTrainBatchSize = 32\n",
    "    hparamTrainNumEpochs = 100\n",
    "    hparamOutputFolder = \"/Users/nikolai/Downloads/UPC/VSC/Project/output/\"\n",
    "    hparamDeviceType = 'cpu'\n",
    "    hparamFeatureTransform = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'type' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m123\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39m#torch.utils.data.random_split(SemanticKittiDataset, [0.2, 0.8], generator=torch.Generator().manual_seed(42))\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_dataset, val_dataset, test_dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mrandom_split(SemanticKittiDataset, [\u001b[39m0.7\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m0.1\u001b[39;49m])\n\u001b[1;32m     24\u001b[0m train_dataset \u001b[39m=\u001b[39m SemanticKittiDataset(\n\u001b[1;32m     25\u001b[0m     dst_hparamDatasetPath\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mhparamDatasetPath[\u001b[39m0\u001b[39m],\n\u001b[1;32m     26\u001b[0m     dst_hparamDatasetSequence\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mhparamDatasetSequence,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     dst_hparamActionType\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m     dst_hparamPointDimension\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m     31\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_dataset))\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataset.py:331\u001b[0m, in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[39mif\u001b[39;00m frac \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m frac \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    329\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFraction at index \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m is not between 0 and 1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m     n_items_in_split \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\n\u001b[0;32m--> 331\u001b[0m         math\u001b[39m.\u001b[39mfloor(\u001b[39mlen\u001b[39;49m(dataset) \u001b[39m*\u001b[39m frac)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m     subset_lengths\u001b[39m.\u001b[39mappend(n_items_in_split)\n\u001b[1;32m    334\u001b[0m remainder \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataset) \u001b[39m-\u001b[39m \u001b[39msum\u001b[39m(subset_lengths)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'type' has no len()"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "#from A0_Configuration.hyperparam import opt\n",
    "from B0_Dataset.dataset import SemanticKittiDataset\n",
    "from D0_Modeling.model import SegmentationPointNet\n",
    "from B1_Dataloader.dataloader import DataLoader_\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "#torch.utils.data.random_split(SemanticKittiDataset, [0.2, 0.8], generator=torch.Generator().manual_seed(42))\n",
    "#train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(SemanticKittiDataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "train_dataset = SemanticKittiDataset(\n",
    "    dst_hparamDatasetPath=opt.hparamDatasetPath[0],\n",
    "    dst_hparamDatasetSequence=opt.hparamDatasetSequence,\n",
    "    dst_hparamYamlConfigPath=opt.hparamYamlConfigPath[0],\n",
    "    dst_hparamNumberOfRandomPoints=opt.hparamNumPoints,\n",
    "    dst_hparamActionType='train',\n",
    "    dst_hparamPointDimension=4)\n",
    "y = next(iter(train_dataset))\n",
    "\n",
    "val_dataset = SemanticKittiDataset(\n",
    "    dst_hparamDatasetPath=opt.hparamDatasetPath[0],\n",
    "    dst_hparamDatasetSequence=opt.hparamDatasetSequence,\n",
    "    dst_hparamYamlConfigPath=opt.hparamYamlConfigPath[0],\n",
    "    dst_hparamNumberOfRandomPoints=opt.hparamNumPoints,\n",
    "    dst_hparamActionType='val',\n",
    "    dst_hparamPointDimension=4)\n",
    "\n",
    "test_dataset = SemanticKittiDataset(\n",
    "    dst_hparamDatasetPath=opt.hparamDatasetPath[0],\n",
    "    dst_hparamDatasetSequence=opt.hparamDatasetSequence,\n",
    "    dst_hparamYamlConfigPath=opt.hparamYamlConfigPath[0],\n",
    "    dst_hparamNumberOfRandomPoints=opt.hparamNumPoints,\n",
    "    dst_hparamActionType='test')\n",
    "\n",
    "train_dataloader = DataLoader_(\n",
    "    dataset = train_dataset,\n",
    "    batch_size=opt.hparamTrainBatchSize,\n",
    "    shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader_(\n",
    "    dataset = val_dataset,\n",
    "    batch_size=opt.hparamBatchSize,\n",
    "    shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader_(\n",
    "    dataset = test_dataset,\n",
    "    batch_size=opt.hparamBatchSize,\n",
    "    shuffle=True)\n",
    "\n",
    "torch.utils.data.random_split(SemanticKittiDataset, [2, 8], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "print('classes', opt.hparamNumberOfClasses)\n",
    "\n",
    "#try:\n",
    "#    os.makedirs(opt.hparamOutputFolder)\n",
    "#except OSError:\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolai/opt/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024])\n",
      "torch.Size([32, 9])\n",
      "torch.Size([32, 3, 3])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 19\u001b[0m pred, trans_feat \u001b[39m=\u001b[39m model(points)\n\u001b[1;32m     20\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, num_classes)\n\u001b[1;32m     21\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)[:, \u001b[39m0\u001b[39m] \u001b[39m#- 1\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/UPC/VSC/Project/Workspace-AD-LiDAR/D0_Modeling/model.py:147\u001b[0m, in \u001b[0;36mSegmentationPointNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    146\u001b[0m     \u001b[39m# x = [32,3,4000]\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     local_global_features, feature_transform \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_pointnet(x) \u001b[39m#local_global_features.shape = [32,1216,4000] & feature_transform.shape = [32,64,64]\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     x \u001b[39m=\u001b[39m local_global_features \u001b[39m#[32, 4000, 1088]\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/UPC/VSC/Project/Workspace-AD-LiDAR/D0_Modeling/model.py:96\u001b[0m, in \u001b[0;36mBasePointNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m input_transform \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_transform(x_tnet) \u001b[39m#[32,3,3]\u001b[39;00m\n\u001b[1;32m     95\u001b[0m x_tnet \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(x_tnet, input_transform)  \u001b[39m# Performs a batch matrix-matrix product [32,4000,3]\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m extra_dim \u001b[39m=\u001b[39m x[:, \u001b[39m3\u001b[39;49m, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# add a new dimension (r)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m x_tnet \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x_tnet, extra_dim], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# x and y concat with z and r (reflection) [32,3,4]\u001b[39;00m\n\u001b[1;32m     98\u001b[0m x_tnet \u001b[39m=\u001b[39m x_tnet\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# [batch = 32, dims = 4, n_points = 4000]\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 1 with size 3"
     ]
    }
   ],
   "source": [
    "num_classes=opt.hparamNumberOfClasses\n",
    "feature_transform=opt.hparamFeatureTransform\n",
    "model = SegmentationPointNet(num_classes, feature_transform)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "model = model.to(opt.hparamDeviceType)\n",
    "\n",
    "num_batch = len(train_dataset) / opt.hparamBatchSize\n",
    "\n",
    "for epoch in range(opt.hparamNumberOfEpochs):\n",
    "    scheduler.step()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        points, target = data\n",
    "        points = points.transpose(2, 1)\n",
    "        points, target = points.to(opt.hparamDeviceType), target.to(opt.hparamDeviceType)\n",
    "        optimizer.zero_grad()\n",
    "        model = model.train()\n",
    "        pred,trans_feat = model(points) # pred.shape=torch.Size([32, 4000, 20]) & target.shape = torch.Size([32, 4000])\n",
    "        pred = pred.view(-1, num_classes) \n",
    "        target = target.view(-1, 1)[:, 0]\n",
    "        #print(pred.size(), target.size())\n",
    "        loss = F.nll_loss(pred, target) #Consecutive_Predictions(Target) pred=[4000*32, 20])\n",
    "        if opt.hparamFeatureTransform:\n",
    "            loss += feature_transform(trans_feat) * 0.001\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "        correct = pred_choice.eq(target.data).cpu().sum()\n",
    "        print('[%d: %d/%d] train loss: %f accuracy: %f' % (epoch, i, num_batch, loss.item(), correct.item()/float(opt.hparamBatchSize * 2500)))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            j, data = next(enumerate(test_dataloader, 0))\n",
    "            points, target = data\n",
    "            points = points.transpose(2, 1)\n",
    "            points, target = points.to(opt.hparamDeviceType), target.to(opt.hparamDeviceType)\n",
    "            model = model.eval()\n",
    "            pred,_ = model(points)\n",
    "            pred = pred.view(-1, num_classes)\n",
    "            target = target.view(-1, 1)[:, 0]\n",
    "            loss = F.nll_loss(pred, target)\n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "            correct = pred_choice.eq(target.data).cpu().sum()\n",
    "            print('[%d: %d/%d] %s loss: %f accuracy: %f' % (epoch, i, num_batch, blue('test'), loss.item(), correct.item()/float(opt.hparamBatchSize * 2500)))\n",
    "\n",
    "    torch.save(model.state_dict(), '%s/seg_model_%s_%d.pth' % (opt.hparamOutputFolder, epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69bb5bf7338e4323a6009980fdae7212bdac3646f7ada4395191108fa1909851"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
