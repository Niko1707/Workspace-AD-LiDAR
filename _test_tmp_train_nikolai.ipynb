{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opt():\n",
    "    hparamDatasetPath = r\"/Users/nikolai/Downloads/UPC/VSC/Project/dataset/sequences\",\n",
    "    hparamYamlConfigPath = \"/Users/nikolai/Downloads/UPC/VSC/Project/Workspace-AD-LiDAR/F0_Visualization/semantic-kitti-api/config/semantic-kitti.yaml\",\n",
    "    hparamNumberOfRandomPoints = 4000\n",
    "    hparamDatasetSequence = '04'    \n",
    "    hparamNumberOfClasses = 4\n",
    "    hparamClassChoice = 'bicycle'\n",
    "    hparamBatchSize = 32\n",
    "    hparamNumberOfEpochs = 250 #TODO: add to config ?\n",
    "    hparamNumberOfWorkers = 4 #TODO: add to config ?\n",
    "    hparamOutputFolder = 'output' #TODO: add to config ?\n",
    "    hparamDeviceType = 'cuda'\n",
    "    hparamFeatureTransform = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'class_choice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m blue \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[94m\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m x \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     19\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m123\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m train_dataset \u001b[39m=\u001b[39m SemanticKittiDataset(\n\u001b[1;32m     22\u001b[0m     data_catalog_path\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mhparamDatasetPath[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m     23\u001b[0m     sequence_number\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mhparamDatasetSequence,\n\u001b[1;32m     24\u001b[0m     yaml_config_path\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mhparamYamlConfigPath[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m     25\u001b[0m     n_points\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mhparamNumberOfRandomPoints,\n\u001b[1;32m     26\u001b[0m     class_choice\u001b[39m=\u001b[39;49m[opt\u001b[39m.\u001b[39;49mhparamClassChoice],\n\u001b[1;32m     27\u001b[0m     action_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m val_dataset \u001b[39m=\u001b[39m SemanticKittiDataset(\n\u001b[1;32m     30\u001b[0m     data_catalog_path\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mhparamDatasetPath[\u001b[39m0\u001b[39m],\n\u001b[1;32m     31\u001b[0m     sequence_number\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mhparamDatasetSequence,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     class_choice\u001b[39m=\u001b[39m[opt\u001b[39m.\u001b[39mhparamClassChoice],\n\u001b[1;32m     35\u001b[0m     action_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m test_dataset \u001b[39m=\u001b[39m SemanticKittiDataset(\n\u001b[1;32m     38\u001b[0m     data_catalog_path\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mhparamDatasetPath[\u001b[39m0\u001b[39m],\n\u001b[1;32m     39\u001b[0m     sequence_number\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mhparamDatasetSequence,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     class_choice\u001b[39m=\u001b[39m[opt\u001b[39m.\u001b[39mhparamClassChoice],\n\u001b[1;32m     43\u001b[0m     action_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'class_choice'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from B0_Dataset.dataset import SemanticKittiDataset\n",
    "from D0_Modeling.model import SegmentationPointNet\n",
    "from B1_Dataloader.dataloader import DataLoader_\n",
    "from A0_Configuration.hyperparam import Parsing\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = SemanticKittiDataset(\n",
    "    data_catalog_path=opt.hparamDatasetPath[0],\n",
    "    sequence_number=opt.hparamDatasetSequence,\n",
    "    yaml_config_path=opt.hparamYamlConfigPath[0],\n",
    "    n_points=opt.hparamNumberOfRandomPoints,\n",
    "    class_choice=[opt.hparamClassChoice],\n",
    "    action_type='train')\n",
    "\n",
    "val_dataset = SemanticKittiDataset(\n",
    "    data_catalog_path=opt.hparamDatasetPath[0],\n",
    "    sequence_number=opt.hparamDatasetSequence,\n",
    "    yaml_config_path=opt.hparamYamlConfigPath[0],\n",
    "    n_points=opt.hparamNumberOfRandomPoints,\n",
    "    class_choice=[opt.hparamClassChoice],\n",
    "    action_type='val')\n",
    "\n",
    "test_dataset = SemanticKittiDataset(\n",
    "    data_catalog_path=opt.hparamDatasetPath[0],\n",
    "    sequence_number=opt.hparamDatasetSequence,\n",
    "    yaml_config_path=opt.hparamYamlConfigPath[0],\n",
    "    n_points=opt.hparamNumberOfRandomPoints,\n",
    "    class_choice=[opt.hparamClassChoice],\n",
    "    action_type='test')\n",
    "\n",
    "#y = next(iter(dataset))  \n",
    "\n",
    "#print('--------------')\n",
    "#print('Single file data: shape: ', y[0].shape)     \n",
    "#print('Single file data: sample: ', y[0][0])     \n",
    "#print('Single file labels: shape: ', y[1].shape) \n",
    "#print('Single file labels: sample: ', y[1][0])         \n",
    "#print('--------------')\n",
    "\n",
    "train_dataloader = DataLoader_(\n",
    "    dataset = train_dataset,\n",
    "    batch_size=opt.hparamBatchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=int(opt.hparamNumberOfWorkers))\n",
    "\n",
    "val_dataloader = DataLoader_(\n",
    "    dataset = val_dataset,\n",
    "    batch_size=opt.hparamBatchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=int(opt.hparamNumberOfWorkers))\n",
    "\n",
    "test_dataloader = DataLoader_(\n",
    "    dataset = test_dataset,\n",
    "    batch_size=opt.hparamBatchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=int(opt.hparamNumberOfWorkers))\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "# num_classes = len(dataset.classes)\n",
    "# print('classes', num_classes)\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.hparamOutputFolder)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolai/opt/miniconda3/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'opt' has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m feature_transform\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mhparamFeatureTransform\n\u001b[1;32m      3\u001b[0m classifier \u001b[39m=\u001b[39m SegmentationPointNet(num_classes, feature_transform)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mif\u001b[39;00m opt\u001b[39m.\u001b[39;49mmodel \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      6\u001b[0m     classifier\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(opt\u001b[39m.\u001b[39mmodel))\n\u001b[1;32m      9\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(classifier\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, betas\u001b[39m=\u001b[39m(\u001b[39m0.9\u001b[39m, \u001b[39m0.999\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'opt' has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "num_classes=opt.hparamNumberOfClasses\n",
    "feature_transform=opt.hparamFeatureTransform\n",
    "classifier = SegmentationPointNet(num_classes, feature_transform)\n",
    "\n",
    "#if opt.model != '':\n",
    "#    classifier.load_state_dict(torch.load(opt.model))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "classifier = classifier.to(opt.hparamDeviceType)\n",
    "#classifier.cuda()\n",
    "\n",
    "num_batch = len(train_dataset) / opt.hparamBatchSize\n",
    "\n",
    "for epoch in range(opt.hparamNumberOfEpochs):\n",
    "    scheduler.step()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        points, target = data\n",
    "        points = points.transpose(2, 1)\n",
    "        points, target = points.to(opt.hparamDeviceType), target.to(opt.hparamDeviceType)\n",
    "        optimizer.zero_grad()\n",
    "        classifier = classifier.train()\n",
    "        pred, trans, trans_feat = classifier(points)\n",
    "        pred = pred.view(-1, num_classes)\n",
    "        target = target.view(-1, 1)[:, 0] - 1\n",
    "        #print(pred.size(), target.size())\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        if opt.hparamFeatureTransform:\n",
    "            loss += feature_transform(trans_feat) * 0.001\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "        correct = pred_choice.eq(target.data).cpu().sum()\n",
    "        print('[%d: %d/%d] train loss: %f accuracy: %f' % (epoch, i, num_batch, loss.item(), correct.item()/float(opt.hparamBatchSize * 2500)))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            j, data = next(enumerate(test_dataloader, 0))\n",
    "            points, target = data\n",
    "            points = points.transpose(2, 1)\n",
    "            points, target = points.to(opt.hparamDeviceType), target.to(opt.hparamDeviceType)\n",
    "            classifier = classifier.eval()\n",
    "            pred, _, _ = classifier(points)\n",
    "            pred = pred.view(-1, num_classes)\n",
    "            target = target.view(-1, 1)[:, 0] - 1\n",
    "            loss = F.nll_loss(pred, target)\n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "            correct = pred_choice.eq(target.data).cpu().sum()\n",
    "            print('[%d: %d/%d] %s loss: %f accuracy: %f' % (epoch, i, num_batch, blue('test'), loss.item(), correct.item()/float(opt.hparamBatchSize * 2500)))\n",
    "\n",
    "    torch.save(classifier.state_dict(), '%s/seg_model_%s_%d.pth' % (opt.hparamOutputFolder, opt.hparamClassChoice, epoch))\n",
    "\n",
    "## benchmark mIOU\n",
    "shape_ious = []\n",
    "for i,data in tqdm(enumerate(test_dataloader, 0)):\n",
    "    points, target = data\n",
    "    points = points.transpose(2, 1)\n",
    "    points, target = points.to(opt.hparamDeviceType), target.to(opt.hparamDeviceType)\n",
    "    classifier = classifier.eval()\n",
    "    pred, _, _ = classifier(points)\n",
    "    pred_choice = pred.data.max(2)[1]\n",
    "\n",
    "    pred_np = pred_choice.cpu().data.numpy()\n",
    "    target_np = target.cpu().data.numpy() - 1\n",
    "\n",
    "    for shape_idx in range(target_np.shape[0]):\n",
    "        parts = range(num_classes)#np.unique(target_np[shape_idx])\n",
    "        part_ious = []\n",
    "        for part in parts:\n",
    "            I = np.sum(np.logical_and(pred_np[shape_idx] == part, target_np[shape_idx] == part))\n",
    "            U = np.sum(np.logical_or(pred_np[shape_idx] == part, target_np[shape_idx] == part))\n",
    "            if U == 0:\n",
    "                iou = 1 #If the union of groundtruth and prediction points is empty, then count part IoU as 1\n",
    "            else:\n",
    "                iou = I / float(U)\n",
    "            part_ious.append(iou)\n",
    "        shape_ious.append(np.mean(part_ious))\n",
    "\n",
    "print(\"mIOU for class {}: {}\".format(opt.hparamClassChoice, np.mean(shape_ious)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B1_Dataloader.dataloader.DataLoader_"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69bb5bf7338e4323a6009980fdae7212bdac3646f7ada4395191108fa1909851"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
